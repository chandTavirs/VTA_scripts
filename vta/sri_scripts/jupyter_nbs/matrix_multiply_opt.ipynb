{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Matrix Multiply Blocking\n",
    "**Author**: [Thierry Moreau](https://homes.cs.washington.edu/~moreau/)\n",
    "\n",
    "This tutorial provides an overview on how to use TVM to map matrix\n",
    "multiplication efficiently on the VTA design.\n",
    "We recommend covering the `basic-mat-mult` tutorial first.\n",
    "\n",
    "In this tutorial, we will demonstrate TVM schedule optimizations to break large\n",
    "neural network operators down onto smaller blocks to achieve computation within\n",
    "limited hardware accelerator resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPC Setup\n",
    "We start by programming the Pynq's FPGA and building its RPC runtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import tvm\n",
    "from tvm import te\n",
    "import vta\n",
    "import numpy as np\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils\n",
    "from vta.testing import simulator\n",
    "\n",
    "# Load VTA parameters from the 3rdparty/vta-hw/config/vta_config.json file\n",
    "env = vta.get_env()\n",
    "\n",
    "# We read the Pynq RPC host IP address and port number from the OS environment\n",
    "host = os.environ.get(\"VTA_RPC_HOST\", \"192.168.2.99\")\n",
    "port = int(os.environ.get(\"VTA_RPC_PORT\", \"9091\"))\n",
    "\n",
    "# We configure both the bitstream and the runtime system on the Pynq\n",
    "# to match the VTA configuration specified by the vta_config.json file.\n",
    "if env.TARGET == \"pynq\":\n",
    "\n",
    "    # Make sure that TVM was compiled with RPC=1\n",
    "    assert tvm.runtime.enabled(\"rpc\")\n",
    "    remote = rpc.connect(host, port)\n",
    "\n",
    "    # Reconfigure the JIT runtime\n",
    "    vta.reconfig_runtime(remote)\n",
    "\n",
    "    # Program the FPGA with a pre-compiled VTA bitstream.\n",
    "    # You can program the FPGA with your own custom bitstream\n",
    "    # by passing the path to the bitstream file instead of None.\n",
    "    vta.program_fpga(remote, bitstream=\"/home/srchand/Desktop/overlays/vta/vta_apm_6slots.bit\")\n",
    "\n",
    "# In simulation mode, host the RPC server locally.\n",
    "elif env.TARGET in [\"sim\", \"tsim\"]:\n",
    "    remote = rpc.LocalSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Declaration\n",
    "As a first step, we need to describe our matrix multiplication computation.\n",
    "We define the matrix multiplication as the computation one would find in a\n",
    "fully connected layer, defined by its batch size, input channels, and output\n",
    "channels.\n",
    "These have to be integer multiples of the VTA tensor shape:\n",
    ":code:`BATCH`, :code:`BLOCK_IN`, and :code:`BLOCK_OUT` respectively.\n",
    "\n",
    "We've added extra operators to the matrix multiplication that apply\n",
    "shifting and clipping to the output in order to mimic a fixed-point\n",
    "matrix multiplication followed by a rectified linear activation.\n",
    "We describe the TVM dataflow graph of the fully connected layer below:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/fc_dataflow.png\" align=\"center\">\n",
    "\n",
    "This computation is intentionally too large to fit onto VTA's on-chip\n",
    "buffers all at once. Therefore in the scheduling phase we'll\n",
    "rely on computation blocking strategies to break the computation down into\n",
    "manageable chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer dimensions: 1024 x 1024\n",
    "batch_size = 1\n",
    "in_channels = 1024\n",
    "out_channels = 1024\n",
    "assert batch_size % env.BATCH == 0\n",
    "assert in_channels % env.BLOCK_IN == 0\n",
    "assert out_channels % env.BLOCK_OUT == 0\n",
    "\n",
    "# Let's derive the tiled input tensor shapes\n",
    "data_shape = (batch_size // env.BATCH, in_channels // env.BLOCK_IN, env.BATCH, env.BLOCK_IN)\n",
    "weight_shape = (\n",
    "    out_channels // env.BLOCK_OUT,\n",
    "    in_channels // env.BLOCK_IN,\n",
    "    env.BLOCK_OUT,\n",
    "    env.BLOCK_IN,\n",
    ")\n",
    "output_shape = (batch_size // env.BATCH, out_channels // env.BLOCK_OUT, env.BATCH, env.BLOCK_OUT)\n",
    "num_ops = in_channels * out_channels * batch_size * 2\n",
    "\n",
    "# Reduction axes\n",
    "ic = te.reduce_axis((0, in_channels // env.BLOCK_IN), name=\"ic\")\n",
    "ic_tns = te.reduce_axis((0, env.BLOCK_IN), name=\"ic_tns\")\n",
    "\n",
    "# Input placeholder tensors\n",
    "data = te.placeholder(data_shape, name=\"data\", dtype=env.inp_dtype)\n",
    "weight = te.placeholder(weight_shape, name=\"weight\", dtype=env.wgt_dtype)\n",
    "\n",
    "# Copy buffers\n",
    "data_buf = te.compute(data_shape, lambda *i: data(*i), \"data_buf\")\n",
    "weight_buf = te.compute(weight_shape, lambda *i: weight(*i), \"weight_buf\")\n",
    "\n",
    "# Declare matrix multiply computation\n",
    "res_gemm = te.compute(\n",
    "    output_shape,\n",
    "    lambda bo, co, bi, ci: te.sum(\n",
    "        data_buf[bo, ic, bi, ic_tns].astype(env.acc_dtype)\n",
    "        * weight_buf[co, ic, ci, ic_tns].astype(env.acc_dtype),\n",
    "        axis=[ic, ic_tns],\n",
    "    ),\n",
    "    name=\"res_gem\",\n",
    ")\n",
    "\n",
    "# Add shift stage for fix-point normalization\n",
    "res_shr = te.compute(output_shape, lambda *i: res_gemm(*i) >> env.INP_WIDTH, name=\"res_shr\")\n",
    "\n",
    "# Apply clipping between (0, input max value)\n",
    "inp_max = (1 << (env.INP_WIDTH - 1)) - 1\n",
    "res_max = te.compute(output_shape, lambda *i: tvm.te.max(res_shr(*i), 0), \"res_max\")\n",
    "res_min = te.compute(output_shape, lambda *i: tvm.te.min(res_max(*i), inp_max), \"res_min\")\n",
    "\n",
    "# Apply typecast to input data type before sending results back\n",
    "res = te.compute(output_shape, lambda *i: res_min(*i).astype(env.inp_dtype), name=\"res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling the Computation\n",
    "We'll look at a set of schedule transformations necessary to map the\n",
    "matrix multiplications onto VTA in an efficient fashion.\n",
    "Those include:\n",
    "\n",
    "- Computation blocking\n",
    "- Lowering to VTA hardware intrinsics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main /* id=94784940451648 */ = primfn(data_1: handle, weight_1: handle, res_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {res: Buffer(res_2: Pointer(int8), int8, [1, 64, 1, 16], []),\n",
      "             data: Buffer(data_2: Pointer(int8), int8, [1, 64, 1, 16], []),\n",
      "             weight: Buffer(weight_2: Pointer(int8), int8, [64, 64, 16, 16], [])}\n",
      "  buffer_map = {data_1: data, weight_1: weight, res_1: res} {\n",
      "  allocate(data_buf: Pointer(global int8), int8, [1024]), storage_scope = global;\n",
      "  allocate(weight_buf: Pointer(global int8), int8, [1048576]), storage_scope = global;\n",
      "  allocate(res_gem: Pointer(global int32), int32, [1024]), storage_scope = global {\n",
      "    for (i1: int32, 0, 64) {\n",
      "      for (i3: int32, 0, 16) {\n",
      "        data_buf[((i1*16) + i3)] = (int8*)data_2[((i1*16) + i3)]\n",
      "      }\n",
      "    }\n",
      "    for (i0: int32, 0, 64) {\n",
      "      for (i1_1: int32, 0, 64) {\n",
      "        for (i2: int32, 0, 16) {\n",
      "          for (i3_1: int32, 0, 16) {\n",
      "            weight_buf[((((i0*16384) + (i1_1*256)) + (i2*16)) + i3_1)] = (int8*)weight_2[((((i0*16384) + (i1_1*256)) + (i2*16)) + i3_1)]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (co: int32, 0, 64) {\n",
      "      for (ci: int32, 0, 16) {\n",
      "        res_gem[((co*16) + ci)] = 0\n",
      "        for (ic: int32, 0, 64) {\n",
      "          for (ic_tns: int32, 0, 16) {\n",
      "            res_gem[((co*16) + ci)] = ((int32*)res_gem[((co*16) + ci)] + (cast(int32, (int8*)data_buf[((ic*16) + ic_tns)])*cast(int32, (int8*)weight_buf[((((co*16384) + (ic*256)) + (ci*16)) + ic_tns)])))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (i1_2: int32, 0, 64) {\n",
      "      for (i3_2: int32, 0, 16) {\n",
      "        res_gem[((i1_2*16) + i3_2)] = @tir.shift_right((int32*)res_gem[((i1_2*16) + i3_2)], 8, dtype=int32)\n",
      "      }\n",
      "    }\n",
      "    for (i1_3: int32, 0, 64) {\n",
      "      for (i3_3: int32, 0, 16) {\n",
      "        res_gem[((i1_3*16) + i3_3)] = max((int32*)res_gem[((i1_3*16) + i3_3)], 0)\n",
      "      }\n",
      "    }\n",
      "    for (i1_4: int32, 0, 64) {\n",
      "      for (i3_4: int32, 0, 16) {\n",
      "        res_gem[((i1_4*16) + i3_4)] = min((int32*)res_gem[((i1_4*16) + i3_4)], 127)\n",
      "      }\n",
      "    }\n",
      "    for (i1_5: int32, 0, 64) {\n",
      "      for (i3_5: int32, 0, 16) {\n",
      "        res_2[((i1_5*16) + i3_5)] = cast(int8, (int32*)res_gem[((i1_5*16) + i3_5)])\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create TVM schedule\n",
    "s = te.create_schedule(res.op)\n",
    "# Let's look at the default TVM schedule\n",
    "print(tvm.lower(s, [data, weight, res], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking the Computation\n",
    "The matrix multiplication is by default too large for activations or weights\n",
    "to fit on VTA's on-chip buffers all at once.\n",
    "We block the (1, 1024) by (1024, 1024) matrix multiplication into\n",
    "smaller (1, 256) by (256, 256) matrix multiplications so the intermediate\n",
    "tensors can fit on the accelerator's on-chip SRAM.\n",
    "This approach is similar to blocking techniques applied to CPUs and GPUs in\n",
    "order to increase cache hit rate.\n",
    "\n",
    "We perform blocking along each axes (the batch axis being untouched since\n",
    "we are performing singe-batch inference).\n",
    "We also leave the inner-most tensorization axes as-is in order to allow\n",
    "TVM to pattern-match tensorization.\n",
    "We show the outcome of blocking on the computation schedule in the diagram\n",
    "below:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/blocking.png\" align=\"center\" width=\"480px\">\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The code after loop splitting and reordering is equivalent to the following\n",
    "  pseudo-code. We ignore the batch axis since we are only performing single-batch\n",
    "  inference in this example:\n",
    "\n",
    "```c\n",
    "for (int oc_out = 0; oc_out < 4; ++oc_out) {\n",
    "  // Initialization loop\n",
    "  for (int oc_inn = 0; oc_inn < 16; ++oc_inn) {\n",
    "   for (int oc_tns = 0; oc_tns < 16; ++oc_tns) {\n",
    "    int j = (oc_out * 16 + oc_inn) * 16 + oc_tns;\n",
    "    C[0][j] = 0;\n",
    "   }\n",
    "  }\n",
    "  for (int ic_out = 0; ic_out < 4; ++ic_out) {\n",
    "   // Block loop\n",
    "   for (int oc_inn = 0; oc_inn < 16; ++oc_inn) {\n",
    "    for (int ic_inn = 0; ic_inn < 16; ++ic_inn) {\n",
    "     // Tensorization loop\n",
    "     for (int oc_tns = 0; oc_tns < 16; ++oc_tns) {\n",
    "      for (int ic_tns = 0; ic_tns < 16; ++ic_tns) {\n",
    "       int i = (ic_out * 16 + ic_inn) * 16 + ic_tns;\n",
    "       int j = (oc_out * 16 + oc_inn) * 16 + oc_tns;\n",
    "       C[0][i] = C[0][i] + A[0][i] * B[j][i];\n",
    "      }\n",
    "     }\n",
    "    }\n",
    "   }\n",
    "  }\n",
    " }\n",
    "}</p></div>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main /* id=94784940533872 */ = primfn(data_1: handle, weight_1: handle, res_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {data: Buffer(data_2: Pointer(int8), int8, [1, 64, 1, 16], []),\n",
      "             res: Buffer(res_2: Pointer(int8), int8, [1, 64, 1, 16], []),\n",
      "             weight: Buffer(weight_2: Pointer(int8), int8, [64, 64, 16, 16], [])}\n",
      "  buffer_map = {data_1: data, weight_1: weight, res_1: res} {\n",
      "  allocate(data_buf: Pointer(global int8), int8, [1024]), storage_scope = global;\n",
      "  allocate(weight_buf: Pointer(global int8), int8, [1048576]), storage_scope = global;\n",
      "  allocate(res_gem: Pointer(global int32), int32, [256]), storage_scope = global {\n",
      "    for (i1: int32, 0, 64) {\n",
      "      for (i3: int32, 0, 16) {\n",
      "        data_buf[((i1*16) + i3)] = (int8*)data_2[((i1*16) + i3)]\n",
      "      }\n",
      "    }\n",
      "    for (i0: int32, 0, 64) {\n",
      "      for (i1_1: int32, 0, 64) {\n",
      "        for (i2: int32, 0, 16) {\n",
      "          for (i3_1: int32, 0, 16) {\n",
      "            weight_buf[((((i0*16384) + (i1_1*256)) + (i2*16)) + i3_1)] = (int8*)weight_2[((((i0*16384) + (i1_1*256)) + (i2*16)) + i3_1)]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (i1.outer: int32, 0, 4) {\n",
      "      for (co.init: int32, 0, 16) {\n",
      "        for (ci.init: int32, 0, 16) {\n",
      "          res_gem[((co.init*16) + ci.init)] = 0\n",
      "        }\n",
      "      }\n",
      "      for (ic.outer: int32, 0, 4) {\n",
      "        for (co: int32, 0, 16) {\n",
      "          for (ic.inner: int32, 0, 16) {\n",
      "            for (ci: int32, 0, 16) {\n",
      "              for (ic_tns: int32, 0, 16) {\n",
      "                res_gem[((co*16) + ci)] = ((int32*)res_gem[((co*16) + ci)] + (cast(int32, (int8*)data_buf[(((ic.outer*256) + (ic.inner*16)) + ic_tns)])*cast(int32, (int8*)weight_buf[((((((i1.outer*262144) + (co*16384)) + (ic.outer*4096)) + (ic.inner*256)) + (ci*16)) + ic_tns)])))\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      for (i1_2: int32, 0, 16) {\n",
      "        for (i3_2: int32, 0, 16) {\n",
      "          res_gem[((i1_2*16) + i3_2)] = @tir.shift_right((int32*)res_gem[((i1_2*16) + i3_2)], 8, dtype=int32)\n",
      "        }\n",
      "      }\n",
      "      for (i1_3: int32, 0, 16) {\n",
      "        for (i3_3: int32, 0, 16) {\n",
      "          res_gem[((i1_3*16) + i3_3)] = max((int32*)res_gem[((i1_3*16) + i3_3)], 0)\n",
      "        }\n",
      "      }\n",
      "      for (i1_4: int32, 0, 16) {\n",
      "        for (i3_4: int32, 0, 16) {\n",
      "          res_gem[((i1_4*16) + i3_4)] = min((int32*)res_gem[((i1_4*16) + i3_4)], 127)\n",
      "        }\n",
      "      }\n",
      "      for (i1.inner: int32, 0, 16) {\n",
      "        for (i3_5: int32, 0, 16) {\n",
      "          res_2[(((i1.outer*256) + (i1.inner*16)) + i3_5)] = cast(int8, (int32*)res_gem[((i1.inner*16) + i3_5)])\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's define tiling sizes (expressed in multiples of VTA tensor shape size)\n",
    "b_block = 1 // env.BATCH\n",
    "i_block = 256 // env.BLOCK_IN\n",
    "o_block = 256 // env.BLOCK_OUT\n",
    "\n",
    "# Tile the output tensor along the batch and output channel dimensions\n",
    "# (since by default we are doing single batch inference, the split along\n",
    "#  the batch dimension has no effect)\n",
    "b, oc, b_tns, oc_tns = s[res].op.axis\n",
    "b_out, b_inn = s[res].split(b, b_block)\n",
    "oc_out, oc_inn = s[res].split(oc, o_block)\n",
    "s[res].reorder(b_out, oc_out, b_inn, oc_inn)\n",
    "\n",
    "# Move intermediate computation into each output compute tile\n",
    "s[res_gemm].compute_at(s[res], oc_out)\n",
    "s[res_shr].compute_at(s[res], oc_out)\n",
    "s[res_max].compute_at(s[res], oc_out)\n",
    "s[res_min].compute_at(s[res], oc_out)\n",
    "\n",
    "# Apply additional loop split along reduction axis (input channel)\n",
    "b_inn, oc_inn, b_tns, oc_tns = s[res_gemm].op.axis\n",
    "ic_out, ic_inn = s[res_gemm].split(ic, i_block)\n",
    "\n",
    "# Reorder axes. We move the ic_out axis all the way out of the GEMM\n",
    "# loop to block along the reduction axis\n",
    "s[res_gemm].reorder(ic_out, b_inn, oc_inn, ic_inn, b_tns, oc_tns, ic_tns)\n",
    "\n",
    "# Let's look at the current TVM schedule after blocking\n",
    "print(tvm.lower(s, [data, weight, res], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowering Copies to DMA Transfers\n",
    "Next we set the buffer scopes to the corresponding on-chip VTA SRAM buffers.\n",
    "We move the load loops into the matrix multiply computation loop to stage\n",
    "memory loads such that they fit in the on-chip SRAM buffers.\n",
    "Finally we annotate the load/store loop outer axes with the DMA copy pragma\n",
    "to perform bulk memory transfers on VTA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scope of SRAM buffers\n",
    "s[data_buf].set_scope(env.inp_scope)\n",
    "s[weight_buf].set_scope(env.wgt_scope)\n",
    "s[res_gemm].set_scope(env.acc_scope)\n",
    "s[res_shr].set_scope(env.acc_scope)\n",
    "s[res_min].set_scope(env.acc_scope)\n",
    "s[res_max].set_scope(env.acc_scope)\n",
    "\n",
    "# Block data and weight cache reads\n",
    "s[data_buf].compute_at(s[res_gemm], ic_out)\n",
    "s[weight_buf].compute_at(s[res_gemm], ic_out)\n",
    "\n",
    "# Use DMA copy pragma on DRAM->SRAM operations\n",
    "s[data_buf].pragma(s[data_buf].op.axis[0], env.dma_copy)\n",
    "s[weight_buf].pragma(s[weight_buf].op.axis[0], env.dma_copy)\n",
    "\n",
    "# Use DMA copy pragma on SRAM->DRAM operation\n",
    "# (this implies that these copies should be performed along b_inn,\n",
    "# or result axis 2)\n",
    "s[res].pragma(s[res].op.axis[2], env.dma_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowering Computation to VTA Compute Intrinsics\n",
    "The last phase is to lower the computation loops down to VTA hardware\n",
    "intrinsics by mapping the matrix multiplication to tensor intrinsics,\n",
    "and mapping the shift, and clipping computation to the vector ALU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main /* id=94784940563680 */ = primfn(data_1: handle, weight_1: handle, res_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {res: Buffer(res_2: Pointer(int8), int8, [1, 64, 1, 16], []),\n",
      "             data: Buffer(data_2: Pointer(int8), int8, [1, 64, 1, 16], []),\n",
      "             weight: Buffer(weight_2: Pointer(int8), int8, [64, 64, 16, 16], [])}\n",
      "  buffer_map = {data_1: data, weight_1: weight, res_1: res} {\n",
      "  @tir.vta.coproc_dep_push(3, 2, dtype=int32)\n",
      "  for (i1.outer: int32, 0, 4) {\n",
      "    attr [IterVar(vta: int32, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "      @tir.vta.coproc_dep_pop(3, 2, dtype=int32)\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushGEMMOp\" {\n",
      "        @tir.call_extern(\"VTAUopLoopBegin\", 16, 1, 0, 0, dtype=int32)\n",
      "        @tir.vta.uop_push(0, 1, 0, 0, 0, 0, 0, 0, dtype=int32)\n",
      "        @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "      }\n",
      "      @tir.vta.coproc_dep_push(2, 1, dtype=int32)\n",
      "    }\n",
      "    for (ic.outer: int32, 0, 4) {\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 1 {\n",
      "        @tir.vta.coproc_dep_pop(2, 1, dtype=int32)\n",
      "        @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), data_2, (ic.outer*16), 16, 1, 16, 0, 0, 0, 0, 0, 2, dtype=int32)\n",
      "        @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), weight_2, ((i1.outer*1024) + (ic.outer*16)), 16, 16, 64, 0, 0, 0, 0, 0, 1, dtype=int32)\n",
      "        @tir.vta.coproc_dep_push(1, 2, dtype=int32)\n",
      "      }\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "        @tir.vta.coproc_dep_pop(1, 2, dtype=int32)\n",
      "        attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushGEMMOp\" {\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 16, 1, 0, 16, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopBegin\", 16, 0, 1, 1, dtype=int32)\n",
      "          @tir.vta.uop_push(0, 0, 0, 0, 0, 0, 0, 0, dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "          @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "        }\n",
      "        @tir.vta.coproc_dep_push(2, 1, dtype=int32)\n",
      "      }\n",
      "    }\n",
      "    @tir.vta.coproc_dep_pop(2, 1, dtype=int32)\n",
      "    attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "        @tir.call_extern(\"VTAUopLoopBegin\", 16, 1, 1, 0, dtype=int32)\n",
      "        @tir.vta.uop_push(1, 0, 0, 0, 0, 3, 1, 8, dtype=int32)\n",
      "        @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "      }\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "        @tir.call_extern(\"VTAUopLoopBegin\", 16, 1, 1, 0, dtype=int32)\n",
      "        @tir.vta.uop_push(1, 0, 0, 0, 0, 1, 1, 0, dtype=int32)\n",
      "        @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "      }\n",
      "      attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "        @tir.call_extern(\"VTAUopLoopBegin\", 16, 1, 1, 0, dtype=int32)\n",
      "        @tir.vta.uop_push(1, 0, 0, 0, 0, 0, 1, 127, dtype=int32)\n",
      "        @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "      }\n",
      "      @tir.vta.coproc_dep_push(2, 3, dtype=int32)\n",
      "    }\n",
      "    attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 3 {\n",
      "      @tir.vta.coproc_dep_pop(2, 3, dtype=int32)\n",
      "      for (i1.inner: int32, 0, 16) {\n",
      "        @tir.call_extern(\"VTAStoreBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), i1.inner, 4, res_2, ((i1.outer*16) + i1.inner), 1, 1, 1, dtype=int32)\n",
      "      }\n",
      "      @tir.vta.coproc_dep_push(3, 2, dtype=int32)\n",
      "    }\n",
      "  }\n",
      "  @tir.vta.coproc_sync(, dtype=int32)\n",
      "  @tir.vta.coproc_dep_pop(3, 2, dtype=int32)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply tensorization over the batch tensor tile axis\n",
    "s[res_gemm].tensorize(b_tns, env.gemm)\n",
    "\n",
    "# Add an ALU pragma over the shift and clipping operations\n",
    "s[res_shr].pragma(s[res_shr].op.axis[0], env.alu)\n",
    "s[res_min].pragma(s[res_min].op.axis[0], env.alu)\n",
    "s[res_max].pragma(s[res_max].op.axis[0], env.alu)\n",
    "\n",
    "# Let's look at the final lowered TVM schedule after lowering memory\n",
    "# loads/stores down to DMA copy intrinsics, and the computation down to\n",
    "# VTA compute intrinsics.\n",
    "print(vta.lower(s, [data, weight, res], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TVM Compilation and Verification\n",
    "After specifying the schedule, we can compile it into a TVM function.\n",
    "We save the module so we can send it over RPC.\n",
    "We run the function and verify it against a numpy implementation to\n",
    "ensure correctness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\nMismatched elements: 379 / 1024 (37%)\nMax absolute difference: 127\nMax relative difference: 126.\n x: array([[[[  0, 127,   0, ...,   0, 127,  35]],\n\n        [[127,  16,   0, ..., 127, 127, 127]],...\n y: array([[[[  0, 127,   0, ...,   0, 127,   0]],\n\n        [[127,  41, 127, ..., 127, 127, 127]],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ff92ee4440c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBLOCK_OUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBLOCK_OUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m ).transpose((0, 2, 1, 3))\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_nd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Print stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not equal\n\nMismatched elements: 379 / 1024 (37%)\nMax absolute difference: 127\nMax relative difference: 126.\n x: array([[[[  0, 127,   0, ...,   0, 127,  35]],\n\n        [[127,  16,   0, ..., 127, 127, 127]],...\n y: array([[[[  0, 127,   0, ...,   0, 127,   0]],\n\n        [[127,  41, 127, ..., 127, 127, 127]],..."
     ]
    }
   ],
   "source": [
    "# Compile the TVM module\n",
    "my_gemm = vta.build(\n",
    "    s, [data, weight, res], tvm.target.Target(\"ext_dev\", host=env.target_host), name=\"my_gemm\"\n",
    ")\n",
    "temp = utils.tempdir()\n",
    "my_gemm.save(temp.relpath(\"gemm.o\"))\n",
    "remote.upload(temp.relpath(\"gemm.o\"))\n",
    "f = remote.load_module(\"gemm.o\")\n",
    "\n",
    "# Get the remote device context\n",
    "ctx = remote.ext_dev(0)\n",
    "\n",
    "# Initialize the data and weight arrays randomly in the int range of (-128, 128]\n",
    "data_np = np.random.randint(-128, 128, size=(batch_size, in_channels)).astype(data.dtype)\n",
    "weight_np = np.random.randint(-128, 128, size=(out_channels, in_channels)).astype(weight.dtype)\n",
    "\n",
    "# Apply packing to the data and weight arrays from a 2D to a 4D packed layout\n",
    "data_packed = data_np.reshape(\n",
    "    batch_size // env.BATCH, env.BATCH, in_channels // env.BLOCK_IN, env.BLOCK_IN\n",
    ").transpose((0, 2, 1, 3))\n",
    "weight_packed = weight_np.reshape(\n",
    "    out_channels // env.BLOCK_OUT, env.BLOCK_OUT, in_channels // env.BLOCK_IN, env.BLOCK_IN\n",
    ").transpose((0, 2, 1, 3))\n",
    "\n",
    "# Format the input/output arrays with tvm.nd.array to the DLPack standard\n",
    "data_nd = tvm.nd.array(data_packed, ctx)\n",
    "weight_nd = tvm.nd.array(weight_packed, ctx)\n",
    "res_nd = tvm.nd.array(np.zeros(output_shape).astype(res.dtype), ctx)\n",
    "\n",
    "# Clear stats\n",
    "if env.TARGET in [\"sim\", \"tsim\"]:\n",
    "    simulator.clear_stats()\n",
    "\n",
    "# Invoke the module to perform the computation\n",
    "f(data_nd, weight_nd, res_nd)\n",
    "\n",
    "# Verify against numpy implementation\n",
    "res_ref = np.dot(data_np.astype(env.acc_dtype), weight_np.T.astype(env.acc_dtype))\n",
    "res_ref = res_ref >> env.INP_WIDTH\n",
    "res_ref = np.clip(res_ref, 0, inp_max)\n",
    "res_ref = res_ref.astype(res.dtype)\n",
    "res_ref = res_ref.reshape(\n",
    "    batch_size // env.BATCH, env.BATCH, out_channels // env.BLOCK_OUT, env.BLOCK_OUT\n",
    ").transpose((0, 2, 1, 3))\n",
    "np.testing.assert_equal(res_ref, res_nd.numpy())\n",
    "\n",
    "# Print stats\n",
    "if env.TARGET in [\"sim\", \"tsim\"]:\n",
    "    sim_stats = simulator.stats()\n",
    "    print(\"Execution statistics:\")\n",
    "    for k, v in sim_stats.items():\n",
    "        print(\"\\t{:<16}: {:>16}\".format(k, v))\n",
    "\n",
    "print(\"Successful blocked matrix multiply test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This tutorial demonstrates how TVM scheduling primitives can achieve\n",
    "computation blocking for a matrix multiplication example.\n",
    "This allows us to map arbitrarily large computation onto limited\n",
    "hardware accelerator resources.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
